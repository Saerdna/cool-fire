Date: 2014-12-01
Title: 监督学习
Category: translate
Tag: self-promote, tech
Slug: supervised_learning
lang: zh

###<center>1.监督学习（翻译完）</center>
* 1.1. 广义线性模型
    * 1.1.1. 最小二乘法
        * 1.1.1.1. 最小二乘法复杂度
    * 1.1.2. 岭回归
        * 1.1.2.1. 岭回归复杂度
        * 1.1.2.2. 参数设置:广义交叉验证
    * 1.1.3. 套索算法
        * 1.1.3.1. 参数设置
        	* 1.1.3.1.1. 交叉验证的使用
        	* 1.1.3.1.2. 基于模型选择的信息标准
    * 1.1.4. 弹性网原理
    * 1.1.5. 多任务的套索算法
    * 1.1.6. 最小角回归
    * 1.1.7. 最小角回归 套索
        * 1.1.7.1. 数学公式
    * 1.1.8. 正交匹配追踪算法 (OMP)
    * 1.1.9. 贝叶斯回归
        * 1.1.9.1. 贝叶斯岭回归
        * 1.1.9.2. 自动相关确定 - ARD
    * 1.1.10. 逻辑回归
    * 1.1.11. 随机梯度下降 - SGD
    * 1.1.12. 感知器
    * 1.1.13. 被动侵略算法
    * 1.1.14. 随机抽样一致:增加异常值的鲁棒性
    * 1.1.15. 多项式回归: 基础函数的扩展线性模型
* 1.2. 支持向量机
    * 1.2.1. 分类
        * 1.2.1.1. 多类分类
        * 1.2.1.2. 评分与概率
        * 1.2.1.3. 不平衡问题
    * 1.2.2. 回归
    * 1.2.3. 密度估计,新颖性检测
    * 1.2.4. 复杂度
    * 1.2.5. 实际中使用技巧
    * 1.2.6. 核函数
        * 1.2.6.1. 自定义核
            * 1.2.6.1.1. 用 Python 函数作为核函数
            * 1.2.6.1.2. 使用 Gram 矩阵
            * 1.2.6.1.3. RBF 内核参数
    * 1.2.7. 数学公式
        * 1.2.7.1. SVC
        * 1.2.7.2. NuSVC
    * 1.2.8. 实际细节
* 1.3. 随机梯度下降
    * 1.3.1. 分类
    * 1.3.2. 回归
    * 1.3.3. 稀疏数据的随机梯度下降
    * 1.3.4. 复杂度
    * 1.3.5. 实际中的使用技巧
    * 1.3.6. 数学公式
        * 1.3.6.1. SGD
    * 1.3.7. 实际细节
* 1.4. 最近邻算法
    * 1.4.1. 无监督的近邻算法
        * 1.4.1.1. 找到最近邻
        * 1.4.1.2. KDTree 和 BallTree 类
    * 1.4.2. 最近邻分类问题
    * 1.4.3. 最近邻回归问题
    * 1.4.4. 最近邻算法
        * 1.4.4.1. 暴力
        * 1.4.4.2. K-D Tree
        * 1.4.4.3. Ball Tree
        * 1.4.4.4. 最近邻算法的选择
        * 1.4.4.5. leaf_size 的影响
    * 1.4.5. 最近质心分类
        * 1.4.5.1. 最近的缩小质心
* 1.5. 高斯过程
    * 1.5.1. 例子
        * 1.5.1.1. 一个回归例子的介绍
        * 1.5.1.2. 数据拟合
    * 1.5.2. 数学公式
        * 1.5.2.1. 开始的假设
        * 1.5.2.2. 最佳线性无偏预测 (BLUP)
        * 1.5.2.3. 经验最佳线性无偏预测 (EBLUP)
    * 1.5.3. 相关模型
    * 1.5.4. 回归模型
    * 1.5.5. 实际细节
* 1.6. 交叉分解
* 1.7. 朴素贝叶斯
    * 1.7.1. 高斯朴素贝叶斯
    * 1.7.2. 多项式朴素贝叶斯
    * 1.7.3. 伯努利朴素贝叶斯
    * 1.7.4. 核心朴素贝叶斯模型拟合
* 1.8. 决策树
    * 1.8.1. 分类
    * 1.8.2. 回归
    * 1.8.3. 多输出问题
    * 1.8.4. 复杂度
    * 1.8.5. 实际中的使用技巧
    * 1.8.6. 树算法: ID3, C4.5, C5.0 and CART
    * 1.8.7. 数学公式
        * 1.8.7.1. 分类标准
        * 1.8.7.2. 回归标准
* 1.9. 所有方法
    * 1.9.1. 套袋元估计
    * 1.9.2. 随机森林
        * 1.9.2.1. 随机森林
        * 1.9.2.2. 绝对随机森林
        * 1.9.2.3. 参数
        * 1.9.2.4. 并行化
        * 1.9.2.5. 特征重要性评价
        * 1.9.2.6. 完全随机森林嵌入
    * 1.9.3. AdaBoost算法
        * 1.9.3.1. 使用方法
    * 1.9.4. 梯度推进树
        * 1.9.4.1. 分类
        * 1.9.4.2. 回归
        * 1.9.4.3. 适配额外的弱学习器
        * 1.9.4.4. 控制树的大小
        * 1.9.4.5. 数学公式
            * 1.9.4.5.1. 损失函数
        * 1.9.4.6. 正规化
            * 1.9.4.6.1. 收缩
            * 1.9.4.6.2. 二次采样
        * 1.9.4.7. 解释
            * 1.9.4.7.1. 特征重要性
            * 1.9.4.7.2. 部分依赖
* 1.10. 多用户多标记算法
    * 1.10.1. 多标记分类格式
    * 1.10.2. One-Vs-The-Rest
        * 1.10.2.1. 多类学习
        * 1.10.2.2. 多标签学习
    * 1.10.3. One-Vs-One
        * 1.10.3.1. 多类学习
    * 1.10.4. 纠错输出编码
        * 1.10.4.1. 多类学习
* 1.11. 特征选择
    * 1.11.1. 移除低方差特征
    * 1.11.2. 单变量特征选择
    * 1.11.3. 递归特征消除
    * 1.11.4. L1-based 特征选择
        * 1.11.4.1. 选择非零系数
        * 1.11.4.2. 随机稀疏模型
    * 1.11.5. Tree-based 特征选择
    * 1.11.6. 特征选择是管道的一部分
* 1.12. 半监督
    * 1.12.1. 标签传递
* 1.13. 线性和二次判定分析
    * 1.13.1. 使用 LDA 降维
    * 1.13.2. 数学观
* 1.14. 保序回归
