<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>cool-fire</title><link>http://saerdna.github.io/</link><description></description><atom:link href="http://saerdna.github.io/feeds/saerdna.rss.xml" rel="self"></atom:link><lastBuildDate>Mon, 01 Dec 2014 00:00:00 +0800</lastBuildDate><item><title>监督学习</title><link>http://saerdna.github.io/posts/translate/supervised_learning-zh.html</link><description>&lt;h3&gt;&lt;center&gt;1.监督学习（翻译中）&lt;/center&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;1.1. 广义线性模型&lt;ul&gt;
&lt;li&gt;1.1.1. 最小二乘法&lt;ul&gt;
&lt;li&gt;1.1.1.1. 最小二乘法复杂度&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.1.2. 岭回归&lt;ul&gt;
&lt;li&gt;1.1.2.1. 岭回归复杂度&lt;/li&gt;
&lt;li&gt;1.1.2.2. 参数设置:广义交叉验证&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.1.3. 套索算法&lt;ul&gt;
&lt;li&gt;1.1.3.1. 参数设置&lt;ul&gt;
&lt;li&gt;1.1.3.1.1. 交叉验证的使用&lt;/li&gt;
&lt;li&gt;1.1.3.1.2. 基于模型选择的信息标准&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.1.4. 弹性网原理&lt;/li&gt;
&lt;li&gt;1.1.5. 多任务的套索算法&lt;/li&gt;
&lt;li&gt;1.1.6. 最小角回归&lt;/li&gt;
&lt;li&gt;1.1.7. 最小角回归 套索&lt;ul&gt;
&lt;li&gt;1.1.7.1. 数学公式&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.1.8. 正交匹配追踪算法 (OMP)&lt;/li&gt;
&lt;li&gt;1.1.9. 贝叶斯回归&lt;ul&gt;
&lt;li&gt;1.1.9.1. 贝叶斯岭回归&lt;/li&gt;
&lt;li&gt;1.1.9.2. 自动相关确定 - ARD&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.1.10. 逻辑回归&lt;/li&gt;
&lt;li&gt;1.1.11. 随机梯度下降 - SGD&lt;/li&gt;
&lt;li&gt;1.1.12. 感知器&lt;/li&gt;
&lt;li&gt;1.1.13. 被动侵略算法&lt;/li&gt;
&lt;li&gt;1.1.14. 随机抽样一致:增加异常值的鲁棒性&lt;/li&gt;
&lt;li&gt;1.1.15. 多项式回归: 基础函数的扩展线性模型&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.2. 支持向量机&lt;ul&gt;
&lt;li&gt;1.2.1. 分类&lt;ul&gt;
&lt;li&gt;1.2.1.1. 多类分类&lt;/li&gt;
&lt;li&gt;1.2.1.2. 评分与概率&lt;/li&gt;
&lt;li&gt;1.2.1.3. 不平衡问题&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.2.2. 回归&lt;/li&gt;
&lt;li&gt;1.2.3. 密度估计,新颖性检测&lt;/li&gt;
&lt;li&gt;1.2.4. 复杂度&lt;/li&gt;
&lt;li&gt;1.2.5. 实际中使用技巧&lt;/li&gt;
&lt;li&gt;1.2.6. 核函数&lt;ul&gt;
&lt;li&gt;1.2.6.1. 自定义核&lt;ul&gt;
&lt;li&gt;1.2.6.1.1. 用 Python 函数作为核函数&lt;/li&gt;
&lt;li&gt;1.2.6.1.2. 使用 Gram 矩阵&lt;/li&gt;
&lt;li&gt;1.2.6.1.3. RBF 内核参数&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.2.7. 数学公式&lt;ul&gt;
&lt;li&gt;1.2.7.1. SVC&lt;/li&gt;
&lt;li&gt;1.2.7.2. NuSVC&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.2.8. 实际细节&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.3. 随机梯度下降&lt;ul&gt;
&lt;li&gt;1.3.1. 分类&lt;/li&gt;
&lt;li&gt;1.3.2. 回归&lt;/li&gt;
&lt;li&gt;1.3.3. 稀疏数据的随机梯度下降&lt;/li&gt;
&lt;li&gt;1.3.4. 复杂度&lt;/li&gt;
&lt;li&gt;1.3.5. 实际中的使用技巧&lt;/li&gt;
&lt;li&gt;1.3.6. 数学公式&lt;ul&gt;
&lt;li&gt;1.3.6.1. SGD&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.3.7. 实际细节&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.4. 最近邻算法&lt;ul&gt;
&lt;li&gt;1.4.1. 无监督的近邻算法&lt;ul&gt;
&lt;li&gt;1.4.1.1. 找到最近邻&lt;/li&gt;
&lt;li&gt;1.4.1.2. KDTree 和 BallTree 类&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.4.2. 最近邻分类问题&lt;/li&gt;
&lt;li&gt;1.4.3. 最近邻回归问题&lt;/li&gt;
&lt;li&gt;1.4.4. 最近邻算法&lt;ul&gt;
&lt;li&gt;1.4.4.1. 暴力&lt;/li&gt;
&lt;li&gt;1.4.4.2. K-D Tree&lt;/li&gt;
&lt;li&gt;1.4.4.3. Ball Tree&lt;/li&gt;
&lt;li&gt;1.4.4.4. 最近邻算法的选择&lt;/li&gt;
&lt;li&gt;1.4.4.5. leaf_size 的影响&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.4.5. 最近质心分类&lt;ul&gt;
&lt;li&gt;1.4.5.1. 最近的缩小质心&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.5. 高斯过程&lt;ul&gt;
&lt;li&gt;1.5.1. 例子&lt;ul&gt;
&lt;li&gt;1.5.1.1. 一个回归例子的介绍&lt;/li&gt;
&lt;li&gt;1.5.1.2. 数据拟合&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.5.2. 数学公式&lt;ul&gt;
&lt;li&gt;1.5.2.1. 开始的假设&lt;/li&gt;
&lt;li&gt;1.5.2.2. 最佳线性无偏预测 (BLUP)&lt;/li&gt;
&lt;li&gt;1.5.2.3. 经验最佳线性无偏预测 (EBLUP)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.5.3. 相关模型&lt;/li&gt;
&lt;li&gt;1.5.4. 回归模型&lt;/li&gt;
&lt;li&gt;1.5.5. 实际细节&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.6. 交叉分解&lt;/li&gt;
&lt;li&gt;1.7. 朴素贝叶斯&lt;ul&gt;
&lt;li&gt;1.7.1. 高斯朴素贝叶斯&lt;/li&gt;
&lt;li&gt;1.7.2. 多项式朴素贝叶斯&lt;/li&gt;
&lt;li&gt;1.7.3. 伯努利朴素贝叶斯&lt;/li&gt;
&lt;li&gt;1.7.4. 核心朴素贝叶斯模型拟合&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.8. 决策树&lt;ul&gt;
&lt;li&gt;1.8.1. 分类&lt;/li&gt;
&lt;li&gt;1.8.2. 回归&lt;/li&gt;
&lt;li&gt;1.8.3. 多输出问题&lt;/li&gt;
&lt;li&gt;1.8.4. 复杂度&lt;/li&gt;
&lt;li&gt;1.8.5. 实际中的使用技巧&lt;/li&gt;
&lt;li&gt;1.8.6. 树算法: ID3, C4.5, C5.0 and CART&lt;/li&gt;
&lt;li&gt;1.8.7. 数学公式&lt;ul&gt;
&lt;li&gt;1.8.7.1. 分类标准&lt;/li&gt;
&lt;li&gt;1.8.7.2. 回归标准&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.9. 所有方法&lt;ul&gt;
&lt;li&gt;1.9.1. Bagging meta-estimator&lt;/li&gt;
&lt;li&gt;1.9.2. Forests of randomized trees&lt;ul&gt;
&lt;li&gt;1.9.2.1. Random Forests&lt;/li&gt;
&lt;li&gt;1.9.2.2. Extremely Randomized Trees&lt;/li&gt;
&lt;li&gt;1.9.2.3. Parameters&lt;/li&gt;
&lt;li&gt;1.9.2.4. Parallelization&lt;/li&gt;
&lt;li&gt;1.9.2.5. Feature importance evaluation&lt;/li&gt;
&lt;li&gt;1.9.2.6. Totally Random Trees Embedding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.9.3. AdaBoost&lt;ul&gt;
&lt;li&gt;1.9.3.1. Usage&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.9.4. Gradient Tree Boosting&lt;ul&gt;
&lt;li&gt;1.9.4.1. Classification&lt;/li&gt;
&lt;li&gt;1.9.4.2. Regression&lt;/li&gt;
&lt;li&gt;1.9.4.3. Fitting additional weak-learners&lt;/li&gt;
&lt;li&gt;1.9.4.4. Controlling the tree size&lt;/li&gt;
&lt;li&gt;1.9.4.5. Mathematical formulation&lt;ul&gt;
&lt;li&gt;1.9.4.5.1. Loss Functions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.9.4.6. Regularization&lt;ul&gt;
&lt;li&gt;1.9.4.6.1. Shrinkage&lt;/li&gt;
&lt;li&gt;1.9.4.6.2. Subsampling&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.9.4.7. Interpretation&lt;ul&gt;
&lt;li&gt;1.9.4.7.1. Feature importance&lt;/li&gt;
&lt;li&gt;1.9.4.7.2. Partial dependence&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.10. Multiclass and multilabel algorithms&lt;ul&gt;
&lt;li&gt;1.10.1. Multilabel classification format&lt;/li&gt;
&lt;li&gt;1.10.2. One-Vs-The-Rest&lt;ul&gt;
&lt;li&gt;1.10.2.1. Multiclass learning&lt;/li&gt;
&lt;li&gt;1.10.2.2. Multilabel learning&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.10.3. One-Vs-One&lt;ul&gt;
&lt;li&gt;1.10.3.1. Multiclass learning&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.10.4. Error-Correcting Output-Codes&lt;ul&gt;
&lt;li&gt;1.10.4.1. Multiclass learning&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.11. Feature selection&lt;ul&gt;
&lt;li&gt;1.11.1. Removing features with low variance&lt;/li&gt;
&lt;li&gt;1.11.2. Univariate feature selection&lt;/li&gt;
&lt;li&gt;1.11.3. Recursive feature elimination&lt;/li&gt;
&lt;li&gt;1.11.4. L1-based feature selection&lt;ul&gt;
&lt;li&gt;1.11.4.1. Selecting non-zero coefficients&lt;/li&gt;
&lt;li&gt;1.11.4.2. Randomized sparse models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.11.5. Tree-based feature selection&lt;/li&gt;
&lt;li&gt;1.11.6. Feature selection as part of a pipeline&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.12. Semi-Supervised&lt;ul&gt;
&lt;li&gt;1.12.1. Label Propagation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.13. Linear and quadratic discriminant analysis&lt;ul&gt;
&lt;li&gt;1.13.1. Dimensionality reduction using LDA&lt;/li&gt;
&lt;li&gt;1.13.2. Mathematical Idea&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1.14. Isotonic regression&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Saerdna</dc:creator><pubDate>Mon, 01 Dec 2014 00:00:00 +0800</pubDate><guid>tag:saerdna.github.io,2014-12-01:posts/translate/supervised_learning-zh.html</guid></item></channel></rss>